{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to SAO\n",
    "\n",
    "In this notebook, I'm going to be referencing many of the topics that I covered in Living Assets Lab post on the [Agentic Web](https://www.livingassets.co/blog/agentic-web). If you haven't read up on this topic, go ahead and take five minutes to read that article. The code in this notebook will make a lot more sense once you've had the context.\n",
    "\n",
    "Enjoy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "- Some python coding knowledge\n",
    "- Background understanding of the Agentic Web\n",
    "- Docker, Docker-Compose\n",
    "\n",
    "## Running this notebook\n",
    "- In the repository, you will find a `docker-compose.yml` configuration that you can use to spin up this experiment on your own.\n",
    "- Use the following commands to run this the way we do:\n",
    "```\n",
    "docker-compose up -d --build la-jupyter \n",
    "```\n",
    "- Check your container's logs for the notebook server URL `docker logs -f CONTAINER_ID`\n",
    "- or visit `http://127.0.0.1:8888/tree`\n",
    "- Select the notebook. This one is called `how_to_sao.ipynb`\n",
    "- When it comes time to run the code, select the local jupyter server and allow connections without a token. Since this setup is intended to be run on a local machine, that should be okay. Depending on your IDE instructions to connect to the jupyter server environment may vary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "First we install all required packages. We are using"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[bert_score](https://github.com/Tiiiger/bert_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install bert-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test our installation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo\n",
    "\n",
    "In this demo, we're going to look at the following things to explain how to SAO. \n",
    "\n",
    "### Objectives\n",
    "- Identify a contextual alignment measurement i.e. a method that we can use to measure how closely aligned a generated text response from an LLM is to the original intent(s) of a user query. \n",
    "- In other words, we're looking for a way to demonstrate how good a generated response is by evaluating whether the AI (an LLM in this case) actually answered the question.\n",
    "- We will then apply this measurement to two queries, one from Perplexity.ai and one from a Living Assets Agent.\n",
    "- Finally, we'll discuss the difference in the context of How to SAO!\n",
    "\n",
    "### The User Query\n",
    "| \"What is search agent optimization?\"\n",
    "- In this example, our query a very simple question. What is search agent optimization? I'm a user who wants to know about this emerging topic, and how can I find out about it?\n",
    "\n",
    "\n",
    "### Null & Alternative Hypothesis\n",
    "- For this experience, our null hypothesis states that there will be no difference between the BertScore of our two agentic responses, the Living Assets Agent and the Perplexity agent. According to this hypothesis, we won't able to measure any difference between the BertScores of the two models.\n",
    "\n",
    "- Our alternative hypothesis is that there is a detectable difference between the BertScore of the two models. We will only consider this alternative hypothesis if we have evidence to reject the null hypothesis.\n",
    "\n",
    "- Regardless of the outcome of this experiment, we will discuss the evidence at the end from both a stastical perspective and from the perspective of any potential real world implications.\n",
    "\n",
    "### What the Agents Know\n",
    "- You may be wondering what context does each Agent have access to. The Living Assets Agent has access to proprietary documents that are not published online. They're internal to Living Assets. Think of these as documents internal to any company though, as it is common practice to not publish everything. single. thing. to the public internet.\n",
    "- The Perplexity Agent has access to whatever information Perplexity AI has been able to scrape from the public web, or from bypassing paywalls on private company websites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json import\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing our Agents response text\n",
    "def load_json_from_file(file_path):\n",
    "    # Function to load JSON from a file\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def print_capped_output(text, max_length=500):\n",
    "    # Define a function to print capped output\n",
    "    if len(text) > max_length:\n",
    "        print(text[:max_length] + '...')\n",
    "    else:\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at each of our agent's responses and compare for ourselves what some of the initial differences might be.\n",
    "\n",
    "Starting with a Living Assets Agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Agent Optimization (SAO) is a strategy designed for the evolving AI-centric digital landscape, particularly to enhance the discoverability and usability of content by AI agents. Unlike traditional SEO, which targets human users' search engine results, SAO focuses on structuring content in a way that AI systems can easily understand and utilize. Here are some key components of SAO:\n",
      "\n",
      "1. **Structuring Content**: Clear hierarchies, semantic markup, and machine-readable formats are essential. ...\n"
     ]
    }
   ],
   "source": [
    "# Load JSON data from a file\n",
    "la_agents_file_path = './static/docs/la_agent_response.json'\n",
    "la_agents_response = load_json_from_file(la_agents_file_path)\n",
    "\n",
    "# Get the raw_output from the agent's response\n",
    "la_agents_raw_output = la_agents_response['1']['raw_output']\n",
    "\n",
    "# Print the capped raw_output\n",
    "print_capped_output(la_agents_raw_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search engine optimization (SEO) is the process of improving a website's visibility and ranking in search engine results pages (SERPs) to increase organic traffic. Here's an overview of what SEO agents do:\n",
      "\n",
      "## Key Responsibilities of SEO Agents\n",
      "\n",
      "SEO agents, often working for SEO companies, perform several crucial tasks to optimize websites for search engines:\n",
      "\n",
      "### Keyword Research and Analysis\n",
      "\n",
      "SEO agents conduct thorough keyword research to identify high-performing keywords that can attract tar...\n"
     ]
    }
   ],
   "source": [
    "# Load JSON data from a file\n",
    "perplexity_file_path = './static/docs/perplexity_response.json'\n",
    "perplexity_response = load_json_from_file(perplexity_file_path)\n",
    "\n",
    "# Get the raw_output from the agent's response\n",
    "perplexity_raw_output = perplexity_response['1']['raw_output']\n",
    "\n",
    "# Print the capped raw_output\n",
    "print_capped_output(perplexity_raw_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obvious Differences\n",
    "- So right away there are some obvious differences between the two search results. The one that should be most apparent is the fact that our user's query was what is **search agent optimization?**. This is a question about a specific topic and it is obvious that Perplexity - which relies on web searches, RAG, web scraping, and the BM25 algorithm - was not able to properly figure out the subject. \n",
    "- It also seems that Perplexity may have missed some of the context to the question, and thus it did not address adjacent topics - which might include queries like \"What are new ways to optimize for the agentic web\" or \"What are the changes happening on the internet insofar as search and query behavior?\"\n",
    "\n",
    "### Living Assets Agent Response\n",
    "- The Living Assets Agent is an ensemble of traditional code functions, statistical models, and LLMs. This network of functionalities works together to understand the context of the question and accesses specific documents. The Agent is also able to access subsets of documents and it uses metadata to understand contextual information.\n",
    "- All of these methods combine \"traditional\" AI (statistics), generative AI, and customized data structures to ensure that questions are addressed directly, efficiently and precisely.\n",
    "- Lastly, our document apparently has a Call to Action (CTA) which is common in sales to provide the user with an ability to take actions on the responses they receive from the Living Assets Agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual Understanding: Analysis of the Two Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03db74c4b4cb4dc685cf373c1e99a2de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22399e1fd0d64402aa85cf9e76c55eea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 5.68 seconds, 0.18 sentences/sec\n",
      "BERTScore: 0.8008\n",
      "BERTPrecision: 0.7565\n",
      "BERTRecall: 0.8507\n"
     ]
    }
   ],
   "source": [
    "query = \"What is search agent optimization?\"\n",
    "generated_result = la_agents_raw_output\n",
    "\n",
    "LA_P, LA_R, LA_F1 = score([generated_result], [query], lang=\"en\", verbose=True)\n",
    "\n",
    "print(f\"BERTScore: {LA_F1.item():.4f}\")\n",
    "print(f\"BERTPrecision: {LA_P.item():.4f}\")\n",
    "print(f\"BERTRecall: {LA_R.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "198ceb9796cf4691a27e4ea2b1fac9c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddb20bfcb93548cfa081921ef615e498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 5.73 seconds, 0.17 sentences/sec\n",
      "BERTScore: 0.7903\n",
      "BERTPrecision: 0.7454\n",
      "BERTRecall: 0.8408\n"
     ]
    }
   ],
   "source": [
    "query = \"What is search agent optimization?\"\n",
    "generated_result = perplexity_raw_output\n",
    "\n",
    "Perp_P, Perp_R, Perp_F1 = score([generated_result], [query], lang=\"en\", verbose=True)\n",
    "\n",
    "print(f\"BERTScore: {Perp_F1.item():.4f}\")\n",
    "print(f\"BERTPrecision: {Perp_P.item():.4f}\")\n",
    "print(f\"BERTRecall: {Perp_R.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Discussion\n",
    "\n",
    "We observed an F1 score of `0.79` for the Perplexity agents response, and a score of `0.80` from the Living Assets agents response. Across our other metrics, Precision and Recall, there is a similar discrepancy of `0.01` between the two agents responses to the User Query. \n",
    "\n",
    "This difference seems small, no?\n",
    "\n",
    "While the difference is only 1%, there is an observable 1% difference between the results of a quantitative assessment of the two model's responses to our User Query.\n",
    "\n",
    "We reject the null hypothesis and state that there is a difference between the two model's responses to our original query in this experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What the Results Tell Us\n",
    "The results demonstrate that while both Agents are able to answer the user query effectively, achieving a rating significantly above the proposed minimum threshold of `.65`, there is one agent that has captured 1% of the contextual nuance that is often an unspoken, rather untyped, portion of user queries to LLM agents.\n",
    "\n",
    "In our earlier discussion, we saw how that 1% can make a huge difference. In the `README.md` attached in this repository, I will include both of the full responses from both agents. It is obvious that the Perplexity Agent, while able to answer the question, gave context that diverged quite a bit from the original user query. This would likely result in a very unsatisfied and frustrated user who got the over confident response that is typical of LLM models and RAG systems, without actually addressing any of their question.\n",
    "\n",
    "The Living Assets Agent was also able to effective answer the question in terms of BertScore. However, the Agent's understanding of context, access to offline content, hierarhical organization, use of accessible tagging in the document, among other things, all combined to generate an overall more targeted and more effective response.\n",
    "\n",
    "## The Cost of Misleading Users\n",
    "In conclusion, while both models achieved high BertScores, the model with the slightly higher score demonstrated a critical ability to capture nuanced details that enhance user interactions. This subtle yet impactful difference means the better-performing model provides responses that are not only accurate but also more contextually appropriate, fostering deeper user engagement. In contrast, the overconfidence of the lesser model, despite appearing correct, can mislead users, potentially reducing conversions and discouraging further exploration. This highlights the importance of not just accuracy, but also the quality and relevance of responses in driving successful user interactions.\n",
    "\n",
    "To elevate your search optimization strategy, consider our new approach that combines machine learning with non-technical solutions, greater specificity, and understanding through Agentic ensembles. Take the next step towards enhancing user engagement and conversion. Visit [LivingAssets.co](https://livingassets.co)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
